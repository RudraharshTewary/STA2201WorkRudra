---
title: "Lab_5_Solved"
format: pdf
editor: visual
---

## Question 1
```{r}
library(tidyverse)
library(rstan)
library(tidybayes)
library(here)
kidiq <- read_rds(here("kidiq.RDS"))
kidiq
```
We proceed to make the following plots about the data:
```{r}
iq_plot <- hist(kidiq$kid_score)
mother_age <- hist(kidiq$mom_age)
kid_mom_hs <- ggplot(data = kidiq, aes(x=kid_score,y=mom_iq,color=mom_hs))+
  geom_point()
iq_plot
mother_age
kid_mom_hs
```
We observe the distributions of kid iq and and the ages of the mothers, and we observe that most mothers in the dataset are young. Not to mention, the plot of kid_score and mom iq against each other parametrized by whether the mother visited high school shows us that most kids in the dataset have mothers who attended high school

```{r}
#| output: false
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 10

# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)

fit <- stan(file = here("code/models/kids2.stan"),
            data = data,
            # reducing the iterations a bit to speed things up
            chains = 3,
            iter = 500)
```

## Question 2

We proceed to implement the new updated sigma in our model:
```{r}
#| output: false
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 0.1
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)

fit1 <- stan(file = here("code/models/kids2.stan"),
            data = data,
            chains = 3,
            iter = 500)
```
```{r}
print(fit)
print(fit1)
```
As we see from the summaries of the models, in the new one, the estimate for mu shifts downward heavily, decreasing by 6 points, while the estimate for sigma increases by 1 point. We get the prior and posterior distribution plots in the next chunk:
```{r}
dsamples <- fit1 |>
            gather_draws(mu, sigma)
dsamples |>
            filter(.variable == "mu") |>
            ggplot(aes(.value, color = "posterior")) +
            geom_density(size = 1) +
            xlim(c(70, 100)) +
            stat_function(fun = dnorm,
            args = list(mean = mu0,
            sd = sigma0),
            aes(colour = 'prior'), size = 1) +
            scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) +
            ggtitle("Prior and posterior for mean test scores") +
            xlab("score")


```

## Question 3
```{r}
#| output: false
X <- as.matrix(kidiq$mom_hs, ncol = 1) # force this to be a matrix
K <- 1
data <- list(y = y, N = length(y),
X =X, K = K)
fit2 <- stan(file = here("code/models/kids3.stan"),
data = data,
iter = 1000)
```
### a)

We evaluate and compare the results of our fit with a linear model in the next code chunk:
```{r}
model <- lm(kid_score ~ mom_hs, data=kidiq)
summary(model)

print(fit2)
```
We see that the coefficient estimates for the intercept and beta 1 by the linear model are very close to our fit object.

### b)

We get the pairs plot from the following chunk:
```{r}
pairs(fit2, pars = c("alpha", "beta[1]"))
```
We see that there is a negative linear relation between our intercept and the coefficient of mom_hs. A possible explanation of this phenomenon might be multicollinearity present in the dataset.

## Question 4

We create a new column containing centered mom iqs and then create a new fit object using that as a covariate in the next code chunk:
```{r}
#| output: false
kidiq$mom_iq_cent <- kidiq$mom_iq - mean(kidiq$mom_iq)
X <- as.matrix(kidiq[, c("mom_hs", "mom_iq_cent")])
K <- 2
data3 <- list(y = y,
N = length(y),
X = X,
K = K
)
fit4 <- stan(file = "code/models/kids3.stan",
data = data3,
iter = 1000)
```

## Question 5

We create a linear model in the next code chunk:
```{r}
model1 <-lm(kid_score ~ mom_hs + mom_iq_cent, data=kidiq)
```

We now check the summary of that model with fit4:
```{r}
summary(model1)
print(fit4)
```

Again, we see that the estimates of our linear model are very close to that of our stan model.

## Question 6

We proceed to extract the fit posterior object and then get posterior estimates for alpha and beta to get the required estimates of scores:
```{r}
fit_obj <- rstan::extract(fit4)
alpha_posterior <- fit_obj$alpha
beta_posterior <- fit_obj$beta
sigma_post <- fit_obj$sigma
beta_1 <- beta_posterior[,1]
beta_2 <- beta_posterior[,2]

post_non_hs <- alpha_posterior +  beta_2 * (110 - mean(kidiq$mom_iq))
post_hs <- alpha_posterior + beta_1 * 1 + beta_2 * (110 - mean(kidiq$mom_iq))

df<- data.frame(
  IQ = c(post_non_hs,post_hs),
  HS = rep(c("HS=0","HS=1"),each = length(post_non_hs)) 
)
ggplot(df, aes(x= IQ, fill=HS))+
  geom_density(alpha=0.5)+
  labs(title = "Plots of posterior estimates of scores by education of mother for mothers who have an IQ of
110")+
  scale_fill_manual(values=c("red","blue"))

```

## Question 7

We proceed to generate a histogram plot for posterior predictive samples of the case for a new kid with a mother who graduated high school and has a IQ of 95
```{r}
posterior_95 <- alpha_posterior + beta_1 + beta_2*(95-mean(kidiq$mom_iq)) + sigma_post

hist(posterior_95,main = "Posterior predictive plot for new kid", xlab="Predicted Scores",col="green")
```